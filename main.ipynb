{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MICRO-452:] Project Report\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b> &nbsp;&emsp;&emsp;&emsp;&emsp;&emsp;Antoine Rol, Ruben Jungius, Hugo Masson, Jadd<br>\n",
    "<b>Supervisors:</b> &nbsp;&emsp;&emsp;&emsp;Prof. Francesco Mondada<br>\n",
    "<b>Due date:</b>  &nbsp;&nbsp;&nbsp;&emsp;&emsp;&emsp;&emsp;December 7th, 2023</p>\n",
    "<b>Presentation date:</b> &nbsp;&nbsp;December 14th, 2023</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Thymhiker  </center></h1>\n",
    "<img src=\"\" style=\"width: 600px;\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1><br><div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><spanclass=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Motion-control\"data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li></ul><ulclass=\"toc-item\"><li><span><a href=\"#Test\" data-toc-modified-id=\"Test-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Test</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;<span>Conclusion</a></ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Test\" data-toc-modified-id=\"Test-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Test</a></span></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;</span>Conclusion</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The narrative behind this mobile robotic project involves a hiker. He has to reach a goal position, which is a refuge defined by a tag. On his way, there are black obstacles, which are volcanoes that the Thymio should avoid. Fortunately, he has a plan where the volcanoes are marked. He just has to decide on the best path to stay safe, then proceed along this route and not forget to dodge rocks that are not marked on the map.\n",
    "This project contains 5 majors parts. \n",
    "\n",
    "- The **vision** : The goal of this part is to obtain the obstacle map and the goal position in offline mode, and then acquire the Thymio's real-time position and orientation.\n",
    "\n",
    "- **Global Path** : Utilizing the obstacle map from the Vision part, this section computes the optimal path for the hiker to stay safe. A visibility graph algorithm will be applied, resulting in a list of path point coordinates.\n",
    "\n",
    "- **Local Naviguation** : This section prevents the hiker from colliding with rocks not on the obstacle map. A neural network will be used for this function\n",
    "\n",
    "- **Motion control** : Using the list of path point coordinates from the Global Navigation part, this part controls the speed of the left and right motors to accurately follow the path.\n",
    "\n",
    "- **Filtering** with Kalman filter : In this section, a Kalman filter will be implemented to achieve a more precise position estimation and orientation. The objective is to maintain reliable estimates of orientation and position even in the absence of the camera signal.\n",
    "\n",
    "Below, you will find a grafcet showing the implementation of each part and their interconnections.\n",
    "\n",
    "<img src=\"ImgRapport\\grafcet.PNG\" style=\"width: 700px;\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Project summary, hardware and choices\n",
    "<a class=\"anchor\" id=\"Project-summary-,-hardware-and-choices\"></a>\n",
    "### Project summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The vision component of our project plays a crucial role in detecting the region of interest (ROI) and end-point, facilitating the seamless interaction between the Thymio robot and its environment. In this section, we detail the vision pipeline, encompassing Aruco marker detection, perspective transformation, and obstacle detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aruco Marker Creation\n",
    "Before delving into the vision pipeline, we commence by designing and crafting markers that will play a pivotal role in the detection process. We have chosen to utilize the Aruco environment due to its ease of use and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_50)\n",
    "\n",
    "id = 47 # This is the identifier of the bookmark, you can change it to whatever you need\n",
    "img_size = 700 # Define the size of the final image\n",
    "marker_img = cv2.aruco.generateImageMarker(aruco_dict, id, img_size)\n",
    "\n",
    "cv2.imwrite('aruco{}.png'.format(id), marker_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Transformation\n",
    "\n",
    "#### ROI definition\n",
    "We must determine the region of interest by leveraging the positions of the four corner markers. The process unfolds as follows:\n",
    "\n",
    "<img src=\"ImgRapport/ROI.jpeg\" style=\"width: 700px;\">\n",
    "\n",
    "#### Transformation matrix\n",
    "To establish an accurate representation of the detected ROI and Thymio's environment, a perspective transformation is applied. This involves computing the transformation matrix based on the detected Aruco markers, enabling the correction of distortions and ensuring a consistent viewpoint.\n",
    "\n",
    "We now proceed to compute the transformation. The objective is to derive the transformation matrix, denoted as M, for seamless utilization in subsequent steps, particularly within the get_ROI function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ vision var ###############################\n",
    "#settings for the aruco : \n",
    "arucoDict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_50)\n",
    "arucoParams = cv2.aruco.DetectorParameters()\n",
    "\n",
    "corner_ids = {\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2,\n",
    "    4:3\n",
    "}\n",
    "\n",
    "## cam settings : 720p -> enough resolution/less latency\n",
    "res_w = 720\n",
    "res_h = 1280\n",
    "\n",
    "\n",
    "############################ function def ###############################\n",
    "\n",
    "def get_pos_aruco(detected, search_id):\n",
    "  #return the center and the 4 corners of the aruco\n",
    "  (corners, ids, rejected) = detected\n",
    "\n",
    "  if ids is not None:\n",
    "    for i, id in enumerate(ids):\n",
    "      corner = corners[i][0]\n",
    "      if id[0] == search_id:\n",
    "        center = (corner[0]+corner[1]+corner[2]+corner[3])/4\n",
    "        return (center, corner)\n",
    "  return (None, None)\n",
    "\n",
    "def perspective_correction(image):\n",
    "    detected = cv2.aruco.detectMarkers(image, arucoDict, parameters=arucoParams)\n",
    "    corners = [None]*4\n",
    "    for (id, idx) in corner_ids.items():\n",
    "      (center, _) = get_pos_aruco(detected, id)\n",
    "      if center is not None:\n",
    "        corners[idx] = center\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    # Do perspective correction\n",
    "    pts1 = np.array([corners[0], corners[1], corners[3], corners[2]])\n",
    "    pts2 = np.float32([[res_h,res_w], [0, res_w], [res_h, 0], [0, 0]])\n",
    "\n",
    "    return cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "def get_ROI(img, M): \n",
    "    return cv2.warpPerspective(img,M,(res_h,res_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we obtain the final image with the corrected perspective:\n",
    "\n",
    "<img src=\"Global_map.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obstacle Detection\n",
    "Having corrected the perspective, our next step involves obstacle detection within the Region of Interest (ROI). Initially, we focus on computing the positions of both the Thymio and the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thymio and destination position finding : \n",
    "For detecting the Thymio, it is essential to compute the center, border, and angle of the Aruco marker. The computation is performed as follows: #todo -> decrire comment compute angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ids\n",
    "thymio_id = 5\n",
    "end_point_id = 8\n",
    "\n",
    "def find_thymio(detected):\n",
    "  # Detect aruco\n",
    "  (center, c) = get_pos_aruco(detected, thymio_id)\n",
    "\n",
    "  if center is None:\n",
    "    return (None, None, None)\n",
    "\n",
    "  #compute the direction : \n",
    "  v1 = c[1] - c[0]\n",
    "  v2 = c[2] - c[3]\n",
    "  dir = (v1 + v2)/2\n",
    "  angle = np.arctan2(dir[1], dir[0])\n",
    "  \n",
    "  return (c, center, angle)\n",
    "\n",
    "def find_end_point(detected):\n",
    "  (center, corners) = get_pos_aruco(detected, end_point_id)\n",
    "  if center is None:\n",
    "    return None\n",
    "  return (corners,center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling the Aruco\n",
    "\n",
    "Next, we proceed to mask the Aruco marker by filling it with white, ensuring it remains undetected by the thresholding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aruco_fill(frame, corners):\n",
    "    if corners is not None:\n",
    "        # Reshape thymio_corners to be able to use cv2\n",
    "        c = np.int32(corners.reshape((4, 1, 2)))\n",
    "\n",
    "        # Fill the shape with white to avoid obstacle detection on the thymio/end_point\n",
    "        cv2.fillPoly(frame, [c], color=(255, 255, 255))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threshold on the red channel of the image is employed to identify obstacles. The thresholding operation simplifies the detection of objects with specific color characteristics. The provided code allows us to create a slider, facilitating the dynamic adjustment of the displayed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import Scale, Button\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tuning_done(root):\n",
    "    # Close the current window\n",
    "    root.destroy()\n",
    "\n",
    "############################ red ###############################\n",
    "\n",
    "def update_parameters_red(red_threshold_slider, image_capture):\n",
    "    # Get the current values from the sliders\n",
    "    red_threshold = red_threshold_slider.get()\n",
    "\n",
    "    # Capture a frame from the webcam\n",
    "    #_, frame = image_capture.read()\n",
    "    frame=image_capture\n",
    "\n",
    "    # Apply the updated parameters and display the updated image\n",
    "    result_image = process_image_red(frame, red_threshold)\n",
    "    cv2.imshow(\"Image with the threshold\", result_image)\n",
    "    #cv2.imwrite(\"Obstacle_map.png\",result_image) --> commented in the report but not in the code\n",
    "\n",
    "def process_image_red(image, red_threshold):\n",
    "    red_channel = image[:, :, 0].copy()  # Create a copy of the red channel\n",
    "    red_channel[red_channel > red_threshold] = 0\n",
    "    _, binary = cv2.threshold(red_channel, 1, 255, cv2.THRESH_BINARY)\n",
    "    return binary\n",
    "\n",
    "def red_binarisation(image_capture):\n",
    "    # Create the main window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Parameter Adjustment\")\n",
    "\n",
    "    # Create sliders for parameter adjustment\n",
    "    red_threshold_slider = Scale(root, from_=0, to=255, label=\"Red Threshold\", orient=\"horizontal\")\n",
    "    red_threshold_slider.set(110)\n",
    "    red_threshold_slider.pack()\n",
    "\n",
    "    update_button = tk.Button(root, text=\"Update Image\", command=lambda: update_parameters_red(red_threshold_slider, image_capture))\n",
    "    update_button.pack()\n",
    "\n",
    "    done_button = Button(root, text=\"Tuning Done\", command=lambda: tuning_done(root))\n",
    "    done_button.pack()\n",
    "\n",
    "    # Display the initial image\n",
    "    #_, initial_frame = image_capture.read()\n",
    "    cv2.imshow(\"Frame with Aruco Filled\", image_capture)\n",
    "\n",
    "    # Start the Tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread('/Users/hugo/Documents/EPFL/Master/Ma2/MobileRoboticsProject/Global_map.png') #change so we can test it\n",
    "\n",
    "# Display the result\n",
    "red_binarisation(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the obstacle detection, we apply multiple filter on it to ensure the the multiple form are convex and relatively  clean. Here is the final image that we only compute once at the start of the program : \n",
    "\n",
    "<img src=\"Obstacle_map_filtered.png\" style=\"width: 700px;\">\n",
    "\n",
    "\n",
    "Within the main algorithm, we calculate the Thymio's position and incorporate a visualization to facilitate the fine-tuning of Thymio's parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Global navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "From the Vision, the global Navigation gets an image with all the obstacles. After enlarging those, to find the paths the robot can acutally drive, a Visiblity Graph is generated and the shortest path is found using the Dijkstra Algorithm. \n",
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/global_1.jpeg\" alt=\"Map From Vision\" width=\"320\"/>\n",
    "<img src=\"ImgRapport/global_2.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "From the Vision, we get the left image and use OpenCV to find the edgepoints from the polygons. As sometimes there are points close to each other found by the algorithm, as well as some small substructures, we use an area and point distance threshold to filter out the points that we dont want to find only the edge points of the Polygons\n",
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/global_3.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "We then enlarge the polygons. Given that all our structures are convex shapes, we simply offset the point using vectors from the adjacent points towards the point we want to offset. If the point moves out of the map that way, we crop it off to the mapsize. \n",
    "\n",
    "Then we find all visible connections by comparing every edge between polygons. If they dont cross any of the polygon outlines, we add them and construct a graph with it. \n",
    "\n",
    "We then run a Dijkstra algorithm on the generated graph to find the shortest path which we want to follow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Local Navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local navigation runs continuously, serving as the Thymio's last line of defense against collisions with unforeseen or vision-unseen obstacles. In this phase, the priority is to prevent any collisions, disregarding the predetermined path provided by global navigation.\n",
    "\n",
    "Obstacle avoidance is executed using a Neural Network. The inputs for the Neural Network are the proximity IR sensors arranged around the Thymio (prox in the code). The outputs consist of speed inputs for the right and left motors (motorL and motorR in the code). Between these outputs, there are weights organized in a 2x5 matrix format (NNW in the code). These weights are designed to assign more significance to obstacles directly in front of the Thymio than those on its sides. This adjustment helps the robot turn more when the obstacle is in front, as opposed to being at the extremities of the robot.\n",
    "\n",
    "To prevent the robot from being repulsed by distant walls, a threshold is implemented. Below this threshold, the robot does not consider the obstacle. Additionally, a Gain is applied to the motor speed values, allowing for the adjustment of the avoidance system's reactivity according to preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LocalAvoidance(prox) : \n",
    "    \n",
    "    NNW = np.array([[2, 3, -4, -3, -2],[-2, -3, -4, 3, 2]])\n",
    "    threshold = 500\n",
    "    Gain = 0.03\n",
    "    obstacle_detected = False\n",
    "    prox_for = np.zeros(5)\n",
    "\n",
    "    for i in range(5):\n",
    "        prox_for[i] = prox[i]\n",
    "        if(prox[i] > threshold) :\n",
    "            obstacle_detected = True\n",
    "\n",
    "    if not(obstacle_detected) :\n",
    "        return 0, 0\n",
    "\n",
    "    elif obstacle_detected :\n",
    "        Y = np.dot(NNW, prox_for) * Gain\n",
    "        motor_L = Y[0] \n",
    "        motor_R = Y[1]\n",
    "        return motor_L, motor_R\n",
    "\n",
    "#Test part : \n",
    "prox_test = [2200,1500, 400, 0,0 ] # sensor values wich correspond to an right sided obstacle\n",
    "print(LocalAvoidance(prox_test))   # output : we avoid the obstacle by turning on the right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is a video of the robot avoiding an obstacle. For the demonstration, the speed motors on both side has been set to 200. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The motion control's primary objective is to determine the motor speeds for the left and right sides based on the robot's position (x, y), orientation (theta), and the array of path coordinates. As a reminder, the global navigation part provides us with a list of points that the Thymio needs to reach.\n",
    "\n",
    "For accurate line following, we have drawn inspiration from steering behavior path following. In this context, steering behaviors encompass a set of algorithms or techniques used to control the movement of autonomous agents or entities in a virtual environment, simulating natural and intelligent motion.\n",
    "\n",
    "Within these techniques, we specifically focus on the \"seek\" concept, which involves moving towards a target point defined on the path. In our code, this target point is referred to as the \"carrot.\" In summary, the path-following algorithm comprises two parts:\n",
    "\n",
    "- Compute the carrot position.\n",
    "- Move toward the carrot using a P controller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Compute the carrot position :**\n",
    "To compute the carrot position, we inspired ourselves from this video : https://www.youtube.com/watch?v=rlZYT-uvmGQ&t=16s \n",
    "\n",
    "(1) Initially, we calculate the position projection, denoted as 'projection,' using the Thymio's orientation and a parameter called d_projection.\n",
    "\n",
    "(2) Subsequently, we create two vectors. Vector A represents the vector from the beginning of the segment to the path point, and vector B is the normalized vector from the beginning of the segment to the end.\n",
    "\n",
    "(3) Next, we perform a scalar projection of A onto B, resulting in the value sp.\n",
    "\n",
    "(4) Finally, we compute the carrot position by projecting sp in the direction of B, starting from the robot's position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Go to the carrot :**\n",
    "This section is responsible for determining the motor speeds based on inputs such as the robot's position, the angle (theta), the carrot position, and the projection.\n",
    "\n",
    "(5) Initially, we set the motor speeds to the same value, representing the speed of the robot when the carrot is directly in front of the Thymio.\n",
    "\n",
    "(6) Subsequently, we calculate the distance between the 'projection' and the carrot.\n",
    "        If the distance is less than the margin, we keep the motor speeds unchanged.\n",
    "        Otherwise, we compute the angle Phi, which is the angle between the Thymio's orientation and the ideal orientation to reach the carrot.\n",
    "                Then, we adjust the motor speeds using a P controller: motorR = motorR - phi * KPangle.\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a schema to make it more simple and the code :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ImgRapport\\Schema_pathfollow.jpg\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDteta = 15\n",
    "KPteta = 80\n",
    "segment_idx = 0\n",
    "old_phi = 0\n",
    "\n",
    "def distance(point1, point2):\n",
    "    return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
    "\n",
    "\n",
    "def vector_compute(point1, point2):\n",
    "    vecteur = (point2[0] - point1[0], point2[1] - point1[1])\n",
    "    return vecteur\n",
    "\n",
    "\n",
    "def go_to_carrot(_position, _carrot, _teta, _margin) : \n",
    "    motorL = 90                 # forward speed\n",
    "    motorR = 90                 \n",
    "    global old_phi\n",
    "    d = distance(_position, _carrot)\n",
    "    vector_carrot = vector_compute(_position, _carrot)\n",
    "    if d > _margin :\n",
    "        phi =  math.atan2(vector_carrot[1],vector_carrot[0]) - _teta\n",
    "        phi = adjust_angle(phi)\n",
    "        # Apply PD controller \n",
    "        motorL = motorL + phi * KPteta + (phi-old_phi)*KDteta\n",
    "        motorR = motorR - phi * KPteta - (phi-old_phi)*KDteta\n",
    "        old_phi = phi\n",
    "    return motorL, motorR\n",
    "\n",
    "def adjust_angle(_angle):\n",
    "\n",
    "    while _angle > math.pi:\n",
    "        _angle -= 2*math.pi\n",
    "    while _angle < -math.pi:\n",
    "        _angle += 2*math.pi\n",
    "    return _angle\n",
    "\n",
    "def follow_path(position, teta, path, path_has_been_done) :\n",
    "\n",
    "    # Initialization of the variables\n",
    "    projection = np.array([0,0])\n",
    "    carrot = np.array([0,0])\n",
    "    motorL = 0\n",
    "    motorR = 0\n",
    "    global segment_idx  #index of the target segment. (the segment wich the robot is following)\n",
    "    \n",
    "    # Set parameters\n",
    "    margin = 5 # margin around the line  \n",
    "    d_projection = 120   # distance from the robot to the projection \n",
    "    has_finished = 0    # flag to know if the robot has reached the last point\n",
    "    limit_distance = 40     # distance in pixels to know if the robot has reached the end of an segment\n",
    "    \n",
    "\n",
    "    # Ckech if the path planning just came to be done \n",
    "    if path_has_been_done == 1 :\n",
    "        segment_idx = 0\n",
    "        path_has_been_done = 0\n",
    "\n",
    "    # Check if the position has reached the end segment point\n",
    "    if distance(position, path[segment_idx +1]) < limit_distance : \n",
    "        segment_idx += 1\n",
    "        print('let s change segemnt')\n",
    "        if segment_idx == len(path)-1 :\n",
    "            has_finished = 1\n",
    "            print('End')\n",
    "\n",
    "    # Check if the Thymio has reached the end \n",
    "    if has_finished == 1:\n",
    "        motorL = 0\n",
    "        motorR = 0\n",
    "        return motorL, motorR, has_finished, carrot\n",
    "    else : \n",
    "        # step(1)\n",
    "        projection = position + np.array([d_projection*math.cos(teta), d_projection*math.sin(teta)])\n",
    "        # step(2)\n",
    "        A = vector_compute(path[segment_idx], projection)\n",
    "        B = vector_compute(path[segment_idx], path[segment_idx+1])\n",
    "        Bnormal = B / np.linalg.norm(B)\n",
    "        #step(3)\n",
    "        sp = abs(np.dot(A,Bnormal))\n",
    "        maxsp = distance(path[segment_idx],path[segment_idx+1])\n",
    "        if sp >maxsp : \n",
    "            sp = maxsp\n",
    "        #step(4)\n",
    "        carrot = path[segment_idx] + Bnormal * sp\n",
    "        \n",
    "        \n",
    "        motorL, motorR = go_to_carrot(position, carrot, teta, margin)\n",
    "        return motorL, motorR, has_finished, carrot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first parameter to adjust is d_projection, which determines the horizon of the Thymio. If it's too short, the Thymio will react at the last moment during turns. If it's too long, the Thymio will be less responsive.\n",
    "\n",
    "The second parameters to tune are KPteta and KDteta, which are experimentally determined. If KPteta too large, the Thymio follow line behavior may oscillate; if it's too low, the Thymio will turn too slowly. \n",
    "\n",
    "Overall, there has to be harmony between KPteta, d_projection, and the Gain from the local avoidance to have a well-functioning line follower and effective local avoidance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII . Test\n",
    "<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX . Conclusion\n",
    "<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
