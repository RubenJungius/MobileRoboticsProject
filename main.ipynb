{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MICRO-452:] Project Report\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b> &nbsp;&emsp;&emsp;&emsp;&emsp;&emsp;Antoine Rol, Ruben Jungius, Hugo Masson, Jadd<br>\n",
    "<b>Supervisors:</b> &nbsp;&emsp;&emsp;&emsp;Prof. Francesco Mondada<br>\n",
    "<b>Due date:</b>  &nbsp;&nbsp;&nbsp;&emsp;&emsp;&emsp;&emsp;December 7th, 2023</p>\n",
    "<b>Presentation date:</b> &nbsp;&nbsp;December 14th, 2023</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Thymhiker  </center></h1>\n",
    "<img src=\"\" style=\"width: 600px;\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1><br>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Project-summary-,-hardware-and-choices\" data-toc-modified-id=\"Project-summary-,-hardware-and-choices-2\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>Project summary, hardware and choices</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li></ul><ul class=\"toc-item\"><li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li></ul><ul class=\"toc-item\"><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Conclusion</a></span></li></ul><ul class=\"toc-item\"><li><a href=\"#Main\" data-toc-modified-id=\"Main-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;</span>Main</a></ul> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The narrative behind this mobile robotic project involves a hiker. He has to reach a goal position, which is a refuge defined by a tag. On his way, there are black obstacles, which are volcanoes that the Thymio should avoid. Fortunately, he has a plan where the volcanoes are marked. He just has to decide on the best path to stay safe, then proceed along this route and not forget to dodge rocks that are not marked on the map.\n",
    "This project contains 5 majors parts. \n",
    "\n",
    "- The **vision** : The goal of this part is to obtain the obstacle map and the goal position in offline mode, and then acquire the Thymio's real-time position and orientation.\n",
    "\n",
    "- **Global Path** : Utilizing the obstacle map from the Vision part, this section computes the optimal path for the hiker to stay safe. A visibility graph algorithm will be applied, resulting in a list of path point coordinates.\n",
    "\n",
    "- **Local Naviguation** : This section prevents the hiker from colliding with rocks not on the obstacle map. A neural network will be used for this function\n",
    "\n",
    "- **Motion control** : Using the list of path point coordinates from the Global Navigation part, this part controls the speed of the left and right motors to accurately follow the path.\n",
    "\n",
    "- **Filtering** with Kalman filter : In this section, a Kalman filter will be implemented to achieve a more precise position estimation and orientation. The objective is to maintain reliable estimates of orientation and position even in the absence of the camera signal.\n",
    "\n",
    "Below, you will find a grafcet showing the implementation of each part and their interconnections.\n",
    "\n",
    "<img src=\"ImgRapport\\grafcet.PNG\" style=\"width: 700px;\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Project summary, hardware and choices\n",
    "<a class=\"anchor\" id=\"Project-summary-,-hardware-and-choices\"></a>\n",
    "### Project summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The vision component of our project plays a crucial role in detecting the region of interest (ROI) and end-point, facilitating the seamless interaction between the Thymio robot and its environment. In this section, we detail the vision pipeline, encompassing Aruco marker detection, perspective transformation, and obstacle detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aruco Marker Creation\n",
    "Before delving into the vision pipeline, we commence by designing and crafting markers that will play a pivotal role in the detection process. We have chosen to utilize the Aruco environment due to its ease of use and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_50)\n",
    "\n",
    "id = 47 # This is the identifier of the bookmark, you can change it to whatever you need\n",
    "img_size = 700 # Define the size of the final image\n",
    "marker_img = cv2.aruco.generateImageMarker(aruco_dict, id, img_size)\n",
    "\n",
    "cv2.imwrite('aruco{}.png'.format(id), marker_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Transformation\n",
    "To establish an accurate representation of the detected ROI and Thymio's environment, a perspective transformation is applied. This involves computing the transformation matrix based on the detected Aruco markers, enabling the correction of distortions and ensuring a consistent viewpoint.\n",
    "\n",
    "We now proceed to compute the transformation. The objective is to derive the transformation matrix, denoted as M, for seamless utilization in subsequent steps, particularly within the get_ROI function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_correction(image):\n",
    "    detected = cv2.aruco.detectMarkers(image, arucoDict, parameters=arucoParams)\n",
    "    corners = [None]*4\n",
    "    for (id, idx) in corner_ids.items():\n",
    "      (center, _) = get_pos_aruco(detected, id)\n",
    "      if center is not None:\n",
    "        corners[idx] = center\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    # Do perspective correction\n",
    "    pts1 = np.array([corners[0], corners[1], corners[3], corners[2]])\n",
    "    pts2 = np.float32([[res_h,res_w], [0, res_w], [res_h, 0], [0, 0]])\n",
    "\n",
    "    return cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "\n",
    "def get_ROI(img, M): \n",
    "    return cv2.warpPerspective(img,M,(res_h,res_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obstacle Detection\n",
    "Having corrected the perspective, our next step involves obstacle detection within the Region of Interest (ROI). Initially, we focus on computing the positions of both the Thymio and the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thymio and destination position finding : \n",
    "For detecting the Thymio, it is essential to compute the center, border, and angle of the Aruco marker. The computation is performed as follows: #todo -> decrire comment compute angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_thymio(detected):\n",
    "  # Detect aruco\n",
    "  (center, c) = get_pos_aruco(detected, thymio_id)\n",
    "\n",
    "  if center is None:\n",
    "    return (None, None, None)\n",
    "\n",
    "  #compute the direction : \n",
    "  v1 = c[1] - c[0]\n",
    "  v2 = c[2] - c[3]\n",
    "  dir = (v1 + v2)/2\n",
    "  angle = np.arctan2(dir[1], dir[0])\n",
    "  \n",
    "  return (c, center, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_point(detected):\n",
    "  (center, corners) = get_pos_aruco(detected, end_point_id)\n",
    "  if center is None:\n",
    "    return None\n",
    "  return (corners,center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threshold on the red channel of the image is employed to identify obstacles. The thresholding operation simplifies the detection of objects with specific color characteristics. Our approach suggests to fine tune the thresholding thanks to a slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_red(image, red_threshold):\n",
    "    red_channel = image[:, :, 0].copy()  # Create a copy of the red channel\n",
    "    red_channel[red_channel > red_threshold] = 0\n",
    "    _, binary = cv2.threshold(red_channel, 1, 255, cv2.THRESH_BINARY)\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vision module, incorporating Aruco marker detection, perspective transformation, and obstacle detection, serves as a critical component in enhancing the Thymio robot's understanding of its environment. This integrated approach enables precise localization, ensuring effective navigation and interaction capabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Global navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "From the Vision, the global Navigation gets an image with all the obstacles. After enlarging those, to find the paths the robot can acutally drive, a Visiblity Graph is generated and the shortest path is found using the Dijkstra Algorithm. \n",
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/global_1.jpeg\" alt=\"Map From Vision\" width=\"320\"/>\n",
    "<img src=\"ImgRapport/global_2.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "From the Vision, we get the left image and use OpenCV to find the edgepoints from the polygons. As sometimes there are points close to each other found by the algorithm, we use an area threshold to filter out the points that we dont want to find only the edge points of the Polygons\n",
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/eroded_img.png\" alt=\"Map From Vision\" width=\"320\"/>\n",
    "</center>\n",
    "\n",
    "We then enlarge the polygons. Given that all our structures are convex shapes, we simply offset the point using vectors from the adjacent points towards the point we want to offset. If the point moves out of the map that way, we crop it off to the mapsize. We then redraw the polygons onto a black canvas, as in the picture above. This is to find out if the enlarged structures are overlapping. We apply OpenCV functions again which search for shapes, but this time we need to take in account that the resulting shapes no longer may be convex.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_point(center, a, b, offset, img_shape):\n",
    "    v1 = vector(a, center)\n",
    "    v1 = scalarmult(v1, 1/length(v1)*offset)\n",
    "    v2 = vector(b, center)\n",
    "    v2 = scalarmult(v2, 1/length(v2)*offset)\n",
    "    new_point = v_sum(v_sum(center, v1), v2)\n",
    "    \n",
    "    # Prevent offset moving point offscreen\n",
    "    new_point[0] = 0 if new_point[0]<0 else new_point[0]\n",
    "    new_point[0] = img_shape[1] if new_point[0]>img_shape[1] else new_point[0]\n",
    "    new_point[1] = 0 if new_point[1]<0 else new_point[1]\n",
    "    new_point[1] = img_shape[0] if new_point[1]>img_shape[0] else new_point[1]\n",
    "    \n",
    "    return new_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have all edgepoints and the polygons drawn, we need to find valid connections to build the graph. To do this, we first errode the enlarged picture, to allow the found edge points to be slightly away from the structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connections(img_enlarged, nodelist, maxx, maxy): # polygons are enlarged polygons\n",
    "    poly_connections = []\n",
    "    connections = []\n",
    "    # Add Polygons\n",
    "    _, binary_image = cv2.threshold(img_enlarged, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Define the kernel size for erosion\n",
    "    kernel_size = 3  # You can adjust this value\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    # Apply erosion to make structures slightly smaller\n",
    "    eroded_image = cv2.erode(binary_image, kernel, iterations=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then go through all point pairs, and by connecting them with a line and checking if the line intersects with the erroded polygons, we can define if a pair of points has a valid edge between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_through_white_area(image, start_point, end_point):\n",
    "    # Assuming the image is a binary image from erosion\n",
    "    white_mask = (image == 255).astype(np.uint8)\n",
    " \n",
    "    # Create an empty mask for the line, ensuring it's uint8 type\n",
    "    line_mask = np.zeros_like(white_mask, dtype=np.uint8)\n",
    "\n",
    "    # Draw the line on the mask\n",
    "    cv2.line(line_mask, start_point, end_point, 1, thickness=1)\n",
    "\n",
    "    # Check if any line pixel is also white in the image\n",
    "    intersection = cv2.bitwise_and(white_mask, line_mask)\n",
    "    return np.any(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/global_3.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "<img src=\"ImgRapport/erroded_path.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "We then run a Dijkstra algorithm on the generated graph to find the shortest path which we want to follow. On the left, a result of an optimal path with all connections is shown. On the right, an example is shown using the erroded polygons."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Local Navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local navigation runs continuously, serving as the Thymio's last line of defense against collisions with unforeseen or vision-unseen obstacles. In this phase, the priority is to prevent any collisions, disregarding the predetermined path provided by global navigation.\n",
    "\n",
    "Obstacle avoidance is executed using a Neural Network. The inputs for the Neural Network are the proximity IR sensors arranged around the Thymio (prox in the code). The outputs consist of speed inputs for the right and left motors (motorL and motorR in the code). Between these outputs, there are weights organized in a 2x5 matrix format (NNW in the code). These weights are designed to assign more significance to obstacles directly in front of the Thymio than those on its sides. This adjustment helps the robot turn more when the obstacle is in front, as opposed to being at the extremities of the robot.\n",
    "\n",
    "To prevent the robot from being repulsed by distant walls, a threshold is implemented. Below this threshold, the robot does not consider the obstacle. Additionally, a Gain is applied to the motor speed values, allowing for the adjustment of the avoidance system's reactivity according to preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LocalAvoidance(prox) : \n",
    "    \n",
    "    NNW = np.array([[2, 3, -4, -3, -2],[-2, -3, -4, 3, 2]])\n",
    "    threshold = 500\n",
    "    Gain = 0.01\n",
    "    obstacle_detected = False\n",
    "    prox_for = np.zeros(5)\n",
    "\n",
    "    for i in range(5):\n",
    "        prox_for[i] = prox[i]\n",
    "        if(prox[i] > threshold) :\n",
    "            obstacle_detected = True\n",
    "\n",
    "    if not(obstacle_detected) :\n",
    "        return 0, 0\n",
    "\n",
    "    elif obstacle_detected :\n",
    "        Y = np.dot(NNW, prox_for) * Gain\n",
    "        motor_L = Y[0] \n",
    "        motor_R = Y[1]\n",
    "        return motor_L, motor_R\n",
    "\n",
    "#Test part : \n",
    "prox_test = [2200,1500, 400, 0,0 ] # sensor values wich correspond to an right sided obstacle\n",
    "print(LocalAvoidance(prox_test))   # output : we avoid the obstacle by turning on the right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is a video of the robot avoiding an obstacle. For the demonstration, the speed motors on both side has been set to 200. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The motion control's primary objective is to determine the motor speeds for the left and right sides based on the robot's position (x, y), orientation (theta), and the array of path coordinates. As a reminder, the global navigation part provides us with a list of points that the Thymio needs to reach.\n",
    "\n",
    "For accurate line following, we have drawn inspiration from steering behavior path following. In this context, steering behaviors encompass a set of algorithms or techniques used to control the movement of autonomous agents or entities in a virtual environment, simulating natural and intelligent motion.\n",
    "\n",
    "Within these techniques, we specifically focus on the \"seek\" concept, which involves moving towards a target point defined on the path. In our code, this target point is referred to as the \"carrot.\" In summary, the path-following algorithm comprises two parts:\n",
    "\n",
    "- Compute the carrot position.\n",
    "- Move toward the carrot using a P controller.\n",
    "\n",
    "\n",
    "**Compute the carrot position :**\n",
    "To compute the carrot position, we inspired ourselves from this video : https://www.youtube.com/watch?v=rlZYT-uvmGQ&t=16s \n",
    "\n",
    "(1) Initially, we calculate the position projection, denoted as 'projection,' using the Thymio's orientation and a parameter called d_projection.\n",
    "\n",
    "(2) Subsequently, we create two vectors. Vector A represents the vector from the beginning of the segment to the path point, and vector B is the normalized vector from the beginning of the segment to the end.\n",
    "\n",
    "(3) Next, we perform a scalar projection of A onto B, resulting in the value sp.\n",
    "\n",
    "(4) Finally, we compute the carrot position by projecting sp in the direction of B, starting from the robot's position.\n",
    "\n",
    "The first parameter to adjust is d_projection, which determines the horizon of the Thymio. If it's too short, the Thymio will react at the last moment during turns. If it's too long, the Thymio will be less responsive.\n",
    "\n",
    "The second parameter is Kpteta, which is experimentally determined. If it's too large, the Kalman filter may oscillate; if it's too low, the Thymio will turn too slowly.\n",
    "\n",
    "\n",
    "**Go to the carrot :**\n",
    "This section is responsible for determining the motor speeds based on inputs such as the robot's position, the angle (theta), the carrot position, and the projection.\n",
    "\n",
    "(5) Initially, we set the motor speeds to the same value, representing the speed of the robot when the carrot is directly in front of the Thymio.\n",
    "\n",
    "(6) Subsequently, we calculate the distance between the 'projection' and the carrot.\n",
    "        If the distance is less than the margin, we keep the motor speeds unchanged.\n",
    "        Otherwise, we compute the angle Phi, which is the angle between the Thymio's orientation and the ideal orientation to reach the carrot.\n",
    "                Then, we adjust the motor speeds using a P controller: motorR = motorR - phi * KPangle.\n",
    "\n",
    "Here is a schema to make it more simple and the code :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ImgRapport\\Schema_pathfollow.jpg\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPteta = 70\n",
    "segment_idx = 0\n",
    "\n",
    "# function to know the distance beetwin 2 points\n",
    "def distance(point1, point2):\n",
    "    \n",
    "    return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
    "\n",
    "# function to compute a vector form 2 points\n",
    "def vector_compute(point1, point2):\n",
    "\n",
    "    vecteur = (point2[0] - point1[0], point2[1] - point1[1])\n",
    "    return vecteur\n",
    "\n",
    "# fucntion to bond an angle beetwin -pi and pi  \n",
    "def adjust_angle(_angle):\n",
    "\n",
    "    while _angle > math.pi:\n",
    "        _angle -= 2*math.pi\n",
    "    while _angle < -math.pi:\n",
    "        _angle += 2*math.pi\n",
    "    return _angle\n",
    "\n",
    "\n",
    "\n",
    "def go_to_carrot(_position, _carrot, _teta, _margin) : \n",
    "\n",
    "    motorL = 0\n",
    "    motorR = 0\n",
    "    motorL = 70\n",
    "    motorR = 70\n",
    "\n",
    "    d = distance(_position, _carrot)    # function to know the distance beetwin 2 points\n",
    "    vector_carrot = vector_compute(_position, _carrot)\n",
    "    if d > _margin :\n",
    "        phi =  math.atan2(vector_carrot[1],vector_carrot[0]) - _teta\n",
    "        phi = adjust_angle(phi)     \n",
    "        motorL = motorL + phi * KPteta\n",
    "        motorR = motorR - phi * KPteta\n",
    "\n",
    "    return motorL, motorR\n",
    "\n",
    "\n",
    "def follow_path(position, teta, path, path_has_been_done) :\n",
    "\n",
    "    # Initialization of the variables\n",
    "    projection = np.array([0,0])\n",
    "    carrot = np.array([0,0])\n",
    "    motorL = 0\n",
    "    motorR = 0\n",
    "    global segment_idx  #index of the target segment. (the segment wich the robot is following)\n",
    "    \n",
    "    # Set parameters\n",
    "    margin = 5 # margin around the line  \n",
    "    d_projection = 30   # distance from the robot to the projection \n",
    "    has_finished = 0    # flag to know if the robot has reached the last point\n",
    "    limit_distance = 40     # distance in pixels to know if the robot has reached the end of an segment\n",
    "    \n",
    "\n",
    "    # Ckech if the path planning just came to be done \n",
    "    if path_has_been_done == 1 :\n",
    "        segment_idx = 0\n",
    "        path_has_been_done = 0\n",
    "\n",
    "    # Check if the position has reached the end segment point\n",
    "    if distance(position, path[segment_idx +1]) < limit_distance : \n",
    "        segment_idx += 1\n",
    "        print('let s change segemnt')\n",
    "        if segment_idx == len(path)-1 :\n",
    "            has_finished = 1\n",
    "            print('End')\n",
    "\n",
    "    # Check if the Thymio has reached the end \n",
    "    if has_finished == 1:\n",
    "        motorL = 0\n",
    "        motorR = 0\n",
    "        return motorL, motorR, has_finished\n",
    "    else : \n",
    "        # step(1)\n",
    "        projection = position + np.array([d_projection*math.cos(teta), d_projection*math.sin(teta)])\n",
    "        # step(2)\n",
    "        A = vector_compute(path[segment_idx], projection)\n",
    "        B = vector_compute(path[segment_idx], path[segment_idx+1])\n",
    "        Bnormal = B / np.linalg.norm(B)\n",
    "        #step(3)\n",
    "        sp = abs(np.dot(A,Bnormal))\n",
    "        maxsp = distance(path[segment_idx],path[segment_idx+1])\n",
    "        if sp >maxsp : \n",
    "            sp = maxsp\n",
    "        #step(4)\n",
    "        carrot = path[segment_idx] + Bnormal * sp\n",
    "        \n",
    "        \n",
    "        motorL, motorR = go_to_carrot(position, carrot, teta, Bnormal, margin)\n",
    "        return motorL, motorR, has_finished\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII . Conclusion\n",
    "<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
