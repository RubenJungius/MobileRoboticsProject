{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MICRO-452:] Project Report\n",
    "\n",
    "\n",
    "\n",
    "<p><b>Authors:</b> &nbsp;&emsp;&emsp;&emsp;&emsp;&emsp;Antoine Rol, Ruben Jungius, Hugo Masson, Jadd<br>\n",
    "<b>Supervisors:</b> &nbsp;&emsp;&emsp;&emsp;Prof. Francesco Mondada<br>\n",
    "<b>Due date:</b>  &nbsp;&nbsp;&nbsp;&emsp;&emsp;&emsp;&emsp;December 7th, 2023</p>\n",
    "<b>Presentation date:</b> &nbsp;&nbsp;December 14th, 2023</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Thymhiker  </center></h1>\n",
    "<img src=\"\" style=\"width: 600px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Introduction</a></span></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Vision\" data-toc-modified-id=\"Vision-3\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Vision</a></span></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Global-navigation\" data-toc-modified-id=\"Global-navigation-4\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Global navigation</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Local-navigation\" data-toc-modified-id=\"Local-navigation-5\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Local navigation</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Filtering\" data-toc-modified-id=\"Filtering-6\"><span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>Filtering</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Motion-control\" data-toc-modified-id=\"Motion-control-7\"><span class=\"toc-item-num\">7.&nbsp;&nbsp;</span>Motion control</a></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Test\" data-toc-modified-id=\"Test-8\"><span class=\"toc-item-num\">8.&nbsp;&nbsp;</span>Test</a></span></li>\n",
    "    </ul>\n",
    "    <ul class=\"toc-item\">\n",
    "        <li><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-9\"><span class=\"toc-item-num\">9.&nbsp;&nbsp;</span>Conclusion</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "<a class=\"anchor\" id=\"Introduction\"></a>\n",
    "<p style='text-align: justify;'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The narrative behind this mobile robotic project involves a hiker. He has to reach a goal position, which is a refuge defined by a tag. On his way, there are black obstacles, which are volcanoes that the Thymio should avoid. Fortunately, he has a plan where the volcanoes are marked. He just has to decide on the best path to stay safe, then proceed along this route and not forget to dodge rocks that are not marked on the map.\n",
    "This project contains 5 majors parts. \n",
    "\n",
    "- **Vision** : The goal of this part is to obtain the obstacle map and the goal position in offline mode, and then acquire the Thymio's real-time position and orientation.\n",
    "\n",
    "- **Global Path** : Utilizing the obstacle map from the Vision part, this section computes the optimal path for the hiker to stay safe. A visibility graph algorithm will be applied, resulting in a list of path point coordinates.\n",
    "\n",
    "- **Local Naviguation** : This section prevents the hiker from colliding with rocks not on the obstacle map. A neural network will be used for this function\n",
    "\n",
    "- **Motion control** : Using the list of path point coordinates from the Global Navigation part, this part controls the speed of the left and right motors to accurately follow the path.\n",
    "\n",
    "- **Filtering** with Kalman filter : In this section, a Kalman filter will be implemented to achieve a more precise position estimation and orientation. The objective is to maintain reliable estimates of orientation and position even in the absence of the camera signal.\n",
    "\n",
    "Below, you will find a grafcet showing the implementation of each part and their interconnections.\n",
    "\n",
    "<img src=\"ImgRapport\\grafcet.PNG\" style=\"width: 700px;\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Vision\n",
    "<a class=\"anchor\" id=\"Vision\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The vision component of our project plays a crucial role in detecting the region of interest (ROI) and end-point, facilitating the seamless interaction between the Thymio robot and its environment. In this section, we detail the vision pipeline, encompassing Aruco marker detection, perspective transformation, and obstacle detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aruco Marker Creation\n",
    "Before delving into the vision pipeline, we commence by designing and crafting markers that will play a pivotal role in the detection process. We have chosen to utilize the Aruco environment due to its ease of use and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_50)\n",
    "\n",
    "id = 47 # This is the identifier of the bookmark, you can change it to whatever you need\n",
    "img_size = 700 # Define the size of the final image\n",
    "marker_img = cv2.aruco.generateImageMarker(aruco_dict, id, img_size)\n",
    "\n",
    "cv2.imwrite('aruco{}.png'.format(id), marker_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Transformation\n",
    "\n",
    "#### ROI definition\n",
    "We must determine the region of interest by leveraging the positions of the four corner markers. The process unfolds as follows:\n",
    "\n",
    "<img src=\"ImgRapport/ROI.jpeg\" style=\"width: 700px;\">\n",
    "\n",
    "#### Transformation matrix\n",
    "To establish an accurate representation of the detected ROI and Thymio's environment, a perspective transformation is applied. This involves computing the transformation matrix based on the detected Aruco markers, enabling the correction of distortions and ensuring a consistent viewpoint.\n",
    "\n",
    "We now proceed to compute the transformation. The objective is to derive the transformation matrix, denoted as M, for seamless utilization in subsequent steps, particularly within the get_ROI function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ vision var ###############################\n",
    "#settings for the aruco : \n",
    "arucoDict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_50)\n",
    "arucoParams = cv2.aruco.DetectorParameters()\n",
    "\n",
    "corner_ids = {\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2,\n",
    "    4:3\n",
    "}\n",
    "\n",
    "## cam settings : 720p -> enough resolution/less latency\n",
    "res_w = 720\n",
    "res_h = 1280\n",
    "\n",
    "\n",
    "############################ function def ###############################\n",
    "\n",
    "def get_pos_aruco(detected, search_id):\n",
    "  #return the center and the 4 corners of the aruco\n",
    "  (corners, ids, rejected) = detected\n",
    "\n",
    "  if ids is not None:\n",
    "    for i, id in enumerate(ids):\n",
    "      corner = corners[i][0]\n",
    "      if id[0] == search_id:\n",
    "        center = (corner[0]+corner[1]+corner[2]+corner[3])/4\n",
    "        return (center, corner)\n",
    "  return (None, None)\n",
    "\n",
    "def perspective_correction(image):\n",
    "    detected = cv2.aruco.detectMarkers(image, arucoDict, parameters=arucoParams)\n",
    "    corners = [None]*4\n",
    "    for (id, idx) in corner_ids.items():\n",
    "      (center, _) = get_pos_aruco(detected, id)\n",
    "      if center is not None:\n",
    "        corners[idx] = center\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    # Do perspective correction\n",
    "    pts1 = np.array([corners[0], corners[1], corners[3], corners[2]])\n",
    "    pts2 = np.float32([[res_h,res_w], [0, res_w], [res_h, 0], [0, 0]])\n",
    "\n",
    "    return cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "def get_ROI(img, M): \n",
    "    return cv2.warpPerspective(img,M,(res_h,res_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we obtain the final image with the corrected perspective:\n",
    "\n",
    "<img src=\"ImgRapport\\Global_map.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obstacle Detection\n",
    "Having corrected the perspective, our next step involves obstacle detection within the Region of Interest (ROI). Initially, we focus on computing the positions of both the Thymio and the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thymio and destination position finding : \n",
    "For detecting the Thymio, it is essential to compute the center, border, and angle of the Aruco marker. The computation is performed as follows: for the position, we take the middle point of the Aruco, and for the angle, we obtain two vectors in the direction of the front of the Thymio from the corners of the Aruco. We then average these vectors to obtain a reliable vector direction for the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ids\n",
    "thymio_id = 5\n",
    "end_point_id = 8\n",
    "\n",
    "def find_thymio(detected):\n",
    "  # Detect aruco\n",
    "  (center, c) = get_pos_aruco(detected, thymio_id)\n",
    "\n",
    "  if center is None:\n",
    "    return (None, None, None)\n",
    "\n",
    "  #compute the direction : \n",
    "  v1 = c[1] - c[0]\n",
    "  v2 = c[2] - c[3]\n",
    "  dir = (v1 + v2)/2\n",
    "  angle = np.arctan2(dir[1], dir[0])\n",
    "  \n",
    "  return (c, center, angle)\n",
    "\n",
    "def find_end_point(detected):\n",
    "  (center, corners) = get_pos_aruco(detected, end_point_id)\n",
    "  if center is None:\n",
    "    return None\n",
    "  return (corners,center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling the Aruco\n",
    "\n",
    "Next, we proceed to mask the Aruco marker by filling it with white, ensuring it remains undetected by the thresholding process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aruco_fill(frame, corners):\n",
    "    if corners is not None:\n",
    "        # Reshape thymio_corners to be able to use cv2\n",
    "        c = np.int32(corners.reshape((4, 1, 2)))\n",
    "\n",
    "        # Fill the shape with white to avoid obstacle detection on the thymio/end_point\n",
    "        cv2.fillPoly(frame, [c], color=(255, 255, 255))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threshold on the red channel of the image is employed to identify obstacles. The thresholding operation simplifies the detection of objects with specific color characteristics. The provided code allows us to create a slider, facilitating the dynamic adjustment of the displayed image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import Scale, Button\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tuning_done(root):\n",
    "    # Close the current window\n",
    "    root.destroy()\n",
    "\n",
    "############################ red ###############################\n",
    "\n",
    "def update_parameters_red(red_threshold_slider, image_capture):\n",
    "    # Get the current values from the sliders\n",
    "    red_threshold = red_threshold_slider.get()\n",
    "\n",
    "    # Capture a frame from the webcam\n",
    "    #_, frame = image_capture.read()\n",
    "    frame=image_capture\n",
    "\n",
    "    # Apply the updated parameters and display the updated image\n",
    "    result_image = process_image_red(frame, red_threshold)\n",
    "    cv2.imshow(\"Image with the threshold\", result_image)\n",
    "    #cv2.imwrite(\"Obstacle_map.png\",result_image) --> commented in the report but not in the code\n",
    "\n",
    "def process_image_red(image, red_threshold):\n",
    "    red_channel = image[:, :, 0].copy()  # Create a copy of the red channel\n",
    "    red_channel[red_channel > red_threshold] = 0\n",
    "    _, binary = cv2.threshold(red_channel, 1, 255, cv2.THRESH_BINARY)\n",
    "    return binary\n",
    "\n",
    "def red_binarisation(image_capture):\n",
    "    # Create the main window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Parameter Adjustment\")\n",
    "\n",
    "    # Create sliders for parameter adjustment\n",
    "    red_threshold_slider = Scale(root, from_=0, to=255, label=\"Red Threshold\", orient=\"horizontal\")\n",
    "    red_threshold_slider.set(110)\n",
    "    red_threshold_slider.pack()\n",
    "\n",
    "    update_button = tk.Button(root, text=\"Update Image\", command=lambda: update_parameters_red(red_threshold_slider, image_capture))\n",
    "    update_button.pack()\n",
    "\n",
    "    done_button = Button(root, text=\"Tuning Done\", command=lambda: tuning_done(root))\n",
    "    done_button.pack()\n",
    "\n",
    "    # Display the initial image\n",
    "    #_, initial_frame = image_capture.read()\n",
    "    cv2.imshow(\"Frame with Aruco Filled\", image_capture)\n",
    "\n",
    "    # Start the Tkinter main loop\n",
    "    root.mainloop()\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread('Aruco_filled.png') #change so we can test it\n",
    "\n",
    "# Display the result\n",
    "red_binarisation(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the obstacle detection, we apply multiple filter on it to ensure that the multiple form are convex and relatively  clean. Here is the final image that we only compute once at the start of the program : \n",
    "\n",
    "<img src=\"ImgRapport\\Obstacle_map_filtered.png\" style=\"width: 700px;\">\n",
    "\n",
    "\n",
    "Within the main algorithm, we calculate the Thymio's position and incorporate a visualization to facilitate the fine-tuning of Thymio's parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Global-navigation\n",
    "<a class=\"anchor\" id=\"Global-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept**\n",
    "From the Vision, the global Navigation gets an image with all the obstacles. After enlarging those, to find the paths the robot can acutally drive, a Visiblity Graph is generated and the shortest path is found using the Dijkstra Algorithm. \n",
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/global_1.jpeg\" alt=\"Map From Vision\" width=\"320\"/>\n",
    "<img src=\"ImgRapport/global_2.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "From the Vision, we get the left image and use OpenCV to find the edgepoints from the polygons. As sometimes there are points close to each other found by the algorithm, we use an area threshold to filter out the points that we dont want to find only the edge points of the Polygons\n",
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/eroded_img.png\" alt=\"Map From Vision\" width=\"320\"/>\n",
    "</center>\n",
    "\n",
    "We then enlarge the polygons. Given that all our structures are convex shapes, we simply offset the point using vectors from the adjacent points towards the point we want to offset. If the point moves out of the map that way, we crop it off to the mapsize. We then redraw the polygons onto a black canvas, as in the picture above. This is to find out if the enlarged structures are overlapping. We apply OpenCV functions again which search for shapes, but this time we need to take in account that the resulting shapes no longer may be convex.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_point(center, a, b, offset, img_shape):\n",
    "    v1 = vector(a, center)\n",
    "    v1 = scalarmult(v1, 1/length(v1)*offset)\n",
    "    v2 = vector(b, center)\n",
    "    v2 = scalarmult(v2, 1/length(v2)*offset)\n",
    "    new_point = v_sum(v_sum(center, v1), v2)\n",
    "    \n",
    "    # Prevent offset moving point offscreen\n",
    "    new_point[0] = 0 if new_point[0]<0 else new_point[0]\n",
    "    new_point[0] = img_shape[1] if new_point[0]>img_shape[1] else new_point[0]\n",
    "    new_point[1] = 0 if new_point[1]<0 else new_point[1]\n",
    "    new_point[1] = img_shape[0] if new_point[1]>img_shape[0] else new_point[1]\n",
    "    \n",
    "    return new_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have all edgepoints and the polygons drawn, we need to find valid connections to build the graph. To do this, we first errode the enlarged picture, to allow the found edge points to be slightly away from the structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connections(img_enlarged, nodelist, maxx, maxy): # polygons are enlarged polygons\n",
    "    poly_connections = []\n",
    "    connections = []\n",
    "    # Add Polygons\n",
    "    _, binary_image = cv2.threshold(img_enlarged, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Define the kernel size for erosion\n",
    "    kernel_size = 3  # You can adjust this value\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "\n",
    "    # Apply erosion to make structures slightly smaller\n",
    "    eroded_image = cv2.erode(binary_image, kernel, iterations=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then go through all point pairs, and by connecting them with a line and checking if the line intersects with the erroded polygons, we can define if a pair of points has a valid edge between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_through_white_area(image, start_point, end_point):\n",
    "    # Assuming the image is a binary image from erosion\n",
    "    white_mask = (image == 255).astype(np.uint8)\n",
    " \n",
    "    # Create an empty mask for the line, ensuring it's uint8 type\n",
    "    line_mask = np.zeros_like(white_mask, dtype=np.uint8)\n",
    "\n",
    "    # Draw the line on the mask\n",
    "    cv2.line(line_mask, start_point, end_point, 1, thickness=1)\n",
    "\n",
    "    # Check if any line pixel is also white in the image\n",
    "    intersection = cv2.bitwise_and(white_mask, line_mask)\n",
    "    return np.any(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"ImgRapport/OptimalPath.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "<img src=\"ImgRapport/erroded_path.png\" alt=\"Map From Vision\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "We then run a Dijkstra algorithm on the generated graph to find the shortest path which we want to follow. On the left, a result of an optimal path with all connections is shown. On the right, an example is shown using the erroded polygons as background."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Local-navigation\n",
    "<a class=\"anchor\" id=\"Local-navigation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local navigation runs continuously, serving as the Thymio's last line of defense against collisions with unforeseen or vision-unseen obstacles. In this phase, the priority is to prevent any collisions, disregarding the predetermined path provided by global navigation.\n",
    "\n",
    "Obstacle avoidance is executed using a Neural Network. The inputs for the Neural Network are the proximity IR sensors arranged around the Thymio (prox in the code). The outputs consist of speed inputs for the right and left motors (motorL and motorR in the code). Between these outputs, there are weights organized in a 2x5 matrix format (NNW in the code). These weights are designed to assign more significance to obstacles directly in front of the Thymio than those on its sides. This adjustment helps the robot turn more when the obstacle is in front, as opposed to being at the extremities of the robot.\n",
    "\n",
    "To prevent the robot from being repulsed by distant walls, a threshold is implemented. Below this threshold, the robot does not consider the obstacle. Additionally, a Gain is applied to the motor speed values, allowing for the adjustment of the avoidance system's reactivity according to preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LocalAvoidance(prox) : \n",
    "    \n",
    "    NNW = np.array([[2, 3, -4, -3, -2],[-2, -3, -4, 3, 2]])\n",
    "    threshold = 500\n",
    "    Gain = 0.03\n",
    "    obstacle_detected = False\n",
    "    prox_for = np.zeros(5)\n",
    "\n",
    "    for i in range(5):\n",
    "        prox_for[i] = prox[i]\n",
    "        if(prox[i] > threshold) :\n",
    "            obstacle_detected = True\n",
    "\n",
    "    if not(obstacle_detected) :\n",
    "        return 0, 0\n",
    "\n",
    "    elif obstacle_detected :\n",
    "        Y = np.dot(NNW, prox_for) * Gain\n",
    "        motor_L = Y[0] \n",
    "        motor_R = Y[1]\n",
    "        return motor_L, motor_R\n",
    "\n",
    "#Test part : \n",
    "prox_test = [2200,1500, 400, 0,0 ] # sensor values wich correspond to an right sided obstacle\n",
    "print(LocalAvoidance(prox_test))   # output : we avoid the obstacle by turning on the right "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Filtering\n",
    "<a class=\"anchor\" id=\"Filtering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Kalman filter is used to fuse the data from the camera with that from the wheels to estimate the position and orientation of the robot at any time. The state vector is [x, y, theta], and the dynamics equations follow:\n",
    "$$ \\mathbf{x} ^ + = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\end{bmatrix} ^ + =  \\mathbf{A} \\cdot \\mathbf{x} + \\mathbf{B \\cdot \\mathbf{u}} + \\mathbf{w} $$ \n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{C} \\cdot \\mathbf{x} + \\mathbf{\\nu} $$ \n",
    "\n",
    "where A and C are the identity matrix, $  B = \\delta t \\begin{bmatrix} \\cos(\\theta) & 0 \\\\ \\sin(\\theta) & 0 \\\\ 0 & 1 \\end{bmatrix} $, $ \\mathbf{u} = \\begin{bmatrix} v \\\\ \\omega \\end{bmatrix} $ with $v$ and $\\omega$ being the linear and angular velocities respectively, and $w$ and $\\nu$ being the process noise with covariance $Q$ and measurement noise with covariance $R$ respectively. $\\delta t$ is the time elapsed between iterations.\n",
    "\n",
    "The following is the kalman code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import termcolor\n",
    "from colorama import Fore, Back, Style\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "\n",
    "class kalman:\n",
    "\n",
    "    def __init__(self, map_length_mm , mean_init):\n",
    "\n",
    "        self.L = 95 # distance between wheels, mm\n",
    "        self.thymio_speed_to_mms =1/3.1 \n",
    "        self.map_length_mm = map_length_mm \n",
    "        self.mean_init = mean_init # use data from camera to initialize the states\n",
    "        self.pixel_to_mm = self.map_length_mm/1020  # length of final image is 1020 pixels\n",
    "        self.mm_to_pixel = 1/self.pixel_to_mm\n",
    "        self.mean =  self.pixel_to_mm * self.mean_init  # get the initial states in mm\n",
    "\n",
    "        self.covar = np.zeros([3,3])\n",
    "        self.A = np.eye(3)\n",
    "        self.C = np.eye(3)\n",
    "\n",
    "        self.q_x = 0.04\n",
    "        self.q_y = 0.04 \n",
    "        self.q_theta = 0.08\n",
    "        self.Q = np.diag([self.q_x , self.q_y , self.q_theta])\n",
    "\n",
    "        self.r_x = 0.01721\n",
    "        self.r_y = 0.00577\n",
    "        self.r_theta = 3.966e-6\n",
    "        self.R = np.diag([self.r_x , self.r_y , self.r_theta])\n",
    "\n",
    "    def kalman_update_pose (self, vision_data , left_speed , right_speed , delta_t): # vision_data is in pixels, speeds in thymio unit\n",
    "\n",
    "        if vision_data is None:  # if camera is hidden\n",
    "            vision_bool = 0 # whether or not to consider the camera data (0 means don't)\n",
    "            vision_position = np.zeros([2,1]) # set to anything except None to avoid errors\n",
    "            vision_theta = 0\n",
    "        else:\n",
    "            vision_bool = 1\n",
    "            vision_position = self.pixel_to_mm * np.array([vision_data[0],vision_data[1]]) # vision_position becomes in mm\n",
    "            vision_theta = vision_data[2]\n",
    "\n",
    "        # prediction\n",
    "        lspeed = left_speed * self.thymio_speed_to_mms # mm/s\n",
    "        rspeed = right_speed * self.thymio_speed_to_mms # mm/s\n",
    "        v = (rspeed + lspeed)/2 # mm/s           \n",
    "        w = (lspeed - rspeed)/self.L  # rad/s\n",
    "        theta = self.mean[2]\n",
    "        u = np.array ([v,w])\n",
    "        B = delta_t * np.array( [[np.cos(theta) , 0] , [np.sin(theta) , 0 ] , [0 , 1]] ) # B changes with theta, so recompute B every iteration        \n",
    "        mean_predicted = self.A @ self.mean + B @ u  # prediction\n",
    "        x_predict = mean_predicted[0] # for plotting at the end the evolution of the predicted data before fusing with vision\n",
    "        y_predict = mean_predicted[1]\n",
    "        theta_predict = mean_predicted[2]\n",
    "        covar_predicted = self.A @ self.covar @ np.transpose(self.A)+ self.Q\n",
    "\n",
    "        # vision\n",
    "        y = ( np.array([float(vision_position[0]),float(vision_position[1]),vision_theta]) )   # change the format of vision_position\n",
    "        innov =  y - self.C @ mean_predicted # innovation = measurements - prediction\n",
    "        St = self.C @ covar_predicted @ np.transpose(self.C) + self.R\n",
    "        Kt = vision_bool * covar_predicted @ np.transpose(self.C) @ np.linalg.inv(St) \n",
    "\n",
    "        # updating estimations and covariance\n",
    "        self.mean = mean_predicted + Kt @ innov # if no vision Kt=0 so only use predicted data\n",
    "        self.mean [2] = (self.mean[2] + math.pi) % (2 * math.pi) - math.pi  # keep theta between -pi and pi\n",
    "        self.covar = ( np.eye(3) - Kt @ self.C ) @ covar_predicted\n",
    "\n",
    "        return x_predict , y_predict , theta_predict        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at its different stages. First is the initialization of the class object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, map_length_mm , mean_init):\n",
    "\n",
    "        self.L = 95 # distance between wheels, mm\n",
    "        self.thymio_speed_to_mms =1/3.1 \n",
    "        self.map_length_mm = map_length_mm \n",
    "        self.mean_init = mean_init # use data from camera to initialize the states\n",
    "        self.pixel_to_mm = self.map_length_mm/1020  # length of final image is 1020 pixels\n",
    "        self.mm_to_pixel = 1/self.pixel_to_mm\n",
    "        self.mean =  self.pixel_to_mm * self.mean_init  # get the initial states in mm\n",
    "\n",
    "        self.q_x = 0.04\n",
    "        self.q_y = 0.04 \n",
    "        self.q_theta = 0.08\n",
    "        self.Q = np.diag([self.q_x , self.q_y , self.q_theta])\n",
    "\n",
    "        self.r_x = 0.01721\n",
    "        self.r_y = 0.00577\n",
    "        self.r_theta = 3.966e-6\n",
    "        self.R = np.diag([self.r_x , self.r_y , self.r_theta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kalman filter estimates the states in SI units (mm and radians), so in order to convert the data from the camera from pixels to mm we need to know the dimensions of the map (map_length_mm) and divide it by the resolution of the cropped image (1020) in order to obtain the conversion factor. Furthermore, to convert the speed of the thymio from its own units (-500 to 500) to mm/s we perform a few runs where we give the thymio a constant velocity and record the time it took to cross a determined distance. The ratio between both units is not the same for all speeds, and since we will operate at a nominal speed of 90, we used the conversion factor we recorded for that exact speed, which is 3.1 . We also pass the initial estimation of the pose (position + orientation), \"mean_init\", which comes from the camera and then gets converted to mm.\n",
    "\n",
    "For the choice of the covariance matrices, Q was taken arbitrarily (q_x and q_y are from Exercise session 8), as for R, we had the thymio stay in its position and recorded the data from the camera from which we computed the variance of each variable and plugged it in for r_x, r_y, and r_theta:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the operating part of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_update_pose (self, vision_data , left_speed , right_speed , delta_t): # vision_data is in pixels, speeds in thymio unit\n",
    "\n",
    "        if vision_data is None:  # if camera is hidden\n",
    "            vision_bool = 0 # whether or not to consider the camera data (0 means don't)\n",
    "            vision_position = np.zeros([2,1]) # set to anything except None to avoid errors\n",
    "            vision_theta = 0\n",
    "        else:\n",
    "            vision_bool = 1\n",
    "            vision_position = self.pixel_to_mm * np.array([vision_data[0],vision_data[1]]) # vision_position becomes in mm\n",
    "            vision_theta = vision_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is used to determine whether or not we have data from the camera. If we do, we opertate normally, otherwise we set vision_bool = 0 which will enable us to update the estimated state vector using only the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction         \n",
    "lspeed = left_speed * self.thymio_speed_to_mms # mm/s\n",
    "rspeed = right_speed * self.thymio_speed_to_mms # mm/s\n",
    "v = (rspeed + lspeed)/2 # mm/s           \n",
    "w = (lspeed - rspeed)/self.L  # rad/s\n",
    "theta = self.mean[2]\n",
    "u = np.array ([v,w])\n",
    "B = delta_t * np.array( [[np.cos(theta) , 0] , [np.sin(theta) , 0 ] , [0 , 1]] ) # B changes with theta, so recompute B every iteration        \n",
    "mean_predicted = self.A @ self.mean + B @ u  # prediction\n",
    "x_predict = mean_predicted[0] # for plotting at the end the evolution of the predicted data before fusing with vision\n",
    "y_predict = mean_predicted[1]\n",
    "theta_predict = mean_predicted[2]\n",
    "covar_predicted = self.A @ self.covar @ np.transpose(self.A)+ self.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction, we use the formulas provided in the slides:\n",
    "$$ \\bar{\\mathbf{x}}_t = \\mathbf{A} \\cdot \\bar{\\mathbf{x}}_{t-1} + \\mathbf{B} \\cdot \\mathbf{u} $$\n",
    "$$ \\bar{\\mathbf{\\Sigma}}_t = \\mathbf{A} \\cdot \\bar{\\mathbf{\\Sigma}}_{t-1} \\cdot \\mathbf{A} ^T + \\mathbf{Q} $$\n",
    "\n",
    "\"mean_predicted\" is the predicted state vector, and \"delta_t\" is the time elapsed from the last call of the function kalman_update_pose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vision\n",
    "y = ( np.array([float(vision_position[0]),float(vision_position[1]),vision_theta]) )   # change the format of vision_position\n",
    "innov =  y - self.C @ mean_predicted # innovation = measurements - prediction\n",
    "St = self.C @ covar_predicted @ np.transpose(self.C) + self.R\n",
    "Kt = vision_bool * covar_predicted @ np.transpose(self.C) @ np.linalg.inv(St) \n",
    "\n",
    "# updating estimations and covariance\n",
    "self.mean = mean_predicted + Kt @ innov # if no vision Kt=0 so only use predicted data\n",
    "self.mean [2] = (self.mean[2] + math.pi) % (2 * math.pi) - math.pi  # keep theta between -pi and pi\n",
    "self.covar = ( np.eye(3) - Kt @ self.C ) @ covar_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable y is just a reformatting of the data from the camera, where vision_position = [x , y], so y = [ x , y , theta]. The innovation \"innov\" is the difference between the predicted and the measured data. We then compute the optimal gain for correcting the prediction: \n",
    "$$ K_t = \\mathbf{\\bar{\\Sigma_t}} \\cdot \\mathbf{C}^T \\cdot \\mathbf{S_t}^ {-1} $$\n",
    "where\n",
    "$$ \\mathbf{ S_t = C \\cdot \\bar{\\Sigma_t} \\cdot C^{T} + Q } $$\n",
    "Notice that $K_t$ is multiplied by vision_bool, such as if there is no data from the camera, $K_t$ will be zero and the final estimation will be equal to the prediction (code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating estimations and covariance\n",
    "self.mean = mean_predicted + Kt @ innov # if no vision Kt=0 so only use predicted data\n",
    "self.mean [2] = (self.mean[2] + math.pi) % (2 * math.pi) - math.pi  # keep theta between -pi and pi\n",
    "self.covar = ( np.eye(3) - Kt @ self.C ) @ covar_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the a posteriori estimation which combines the prediction with the data from the camera:\n",
    "$$ \\mathbf{x_t = \\bar{x_t} + K_t i_t} $$\n",
    "where $i$ is the innovation. As for the covariance:\n",
    "$$ \\mathbf{\\Sigma_t = (I - K_t C) \\bar{\\Sigma_t} }$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Motion Control\n",
    "<a class=\"anchor\" id=\"Motion-control\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The motion control's primary objective is to determine the motor speeds for the left and right sides based on the robot's position (x, y), orientation (theta), and the array of path coordinates. As a reminder, the global navigation part provides us with a list of points that the Thymio needs to reach.\n",
    "\n",
    "For accurate line following, we have drawn inspiration from steering behavior path following. In this context, steering behaviors encompass a set of algorithms or techniques used to control the movement of autonomous agents or entities in a virtual environment, simulating natural and intelligent motion.\n",
    "\n",
    "Within these techniques, we specifically focus on the \"seek\" concept, which involves moving towards a target point defined on the path. In our code, this target point is referred to as the \"carrot.\" In summary, the path-following algorithm comprises two parts:\n",
    "\n",
    "- Compute the carrot position.\n",
    "- Move toward the carrot using a PD controller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Compute the carrot position :**\n",
    "To compute the carrot position, we inspired ourselves from this video : https://www.youtube.com/watch?v=rlZYT-uvmGQ&t=16s \n",
    "\n",
    "(1) Initially, we calculate the position projection, denoted as 'projection,' using the Thymio's orientation and a parameter called d_projection.\n",
    "\n",
    "(2) Subsequently, we create two vectors. Vector A represents the vector from the beginning of the segment to the path point, and vector B is the normalized vector from the beginning of the segment to the end.\n",
    "\n",
    "(3) Next, we perform a scalar projection of A onto B, resulting in the value sp.\n",
    "\n",
    "(4) Finally, we compute the carrot position by projecting sp in the direction of B, starting from the robot's position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Go to the carrot :**\n",
    "This section is responsible for determining the motor speeds based on inputs such as the robot's position, the angle (theta), the carrot position, and the projection.\n",
    "\n",
    "(5) Initially, we set the motor speeds to the same value, representing the speed of the robot when the carrot is directly in front of the Thymio.\n",
    "\n",
    "(6) Subsequently, we calculate the distance between the 'projection' and the carrot.\n",
    "        If the distance is less than the margin, we keep the motor speeds unchanged.\n",
    "        Otherwise, we compute the angle Phi, which is the angle between the Thymio's orientation and the ideal orientation to reach the carrot.\n",
    "                Then, we adjust the motor speeds using a PD controller : motorR = motorR - phi * KPangle.\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a schema to make it more simple and the code :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ImgRapport\\Schema_pathfollow.jpg\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDteta = 15\n",
    "KPteta = 80\n",
    "segment_idx = 0\n",
    "old_phi = 0\n",
    "\n",
    "def distance(point1, point2):\n",
    "    return math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2)\n",
    "\n",
    "\n",
    "def vector_compute(point1, point2):\n",
    "    vecteur = (point2[0] - point1[0], point2[1] - point1[1])\n",
    "    return vecteur\n",
    "\n",
    "\n",
    "def go_to_carrot(_position, _carrot, _teta, _margin) : \n",
    "    motorL = 90                 # forward speed\n",
    "    motorR = 90                 \n",
    "    global old_phi\n",
    "    d = distance(_position, _carrot)\n",
    "    vector_carrot = vector_compute(_position, _carrot)\n",
    "    if d > _margin :\n",
    "        phi =  math.atan2(vector_carrot[1],vector_carrot[0]) - _teta\n",
    "        phi = adjust_angle(phi)\n",
    "        # Apply PD controller \n",
    "        motorL = motorL + phi * KPteta + (phi-old_phi)*KDteta\n",
    "        motorR = motorR - phi * KPteta - (phi-old_phi)*KDteta\n",
    "        old_phi = phi\n",
    "    return motorL, motorR\n",
    "\n",
    "def adjust_angle(_angle):\n",
    "\n",
    "    while _angle > math.pi:\n",
    "        _angle -= 2*math.pi\n",
    "    while _angle < -math.pi:\n",
    "        _angle += 2*math.pi\n",
    "    return _angle\n",
    "\n",
    "def follow_path(position, teta, path, path_has_been_done) :\n",
    "\n",
    "    # Initialization of the variables\n",
    "    projection = np.array([0,0])\n",
    "    carrot = np.array([0,0])\n",
    "    motorL = 0\n",
    "    motorR = 0\n",
    "    global segment_idx  #index of the target segment. (the segment wich the robot is following)\n",
    "    \n",
    "    # Set parameters\n",
    "    margin = 5 # margin around the line  \n",
    "    d_projection = 120   # distance from the robot to the projection \n",
    "    has_finished = 0    # flag to know if the robot has reached the last point\n",
    "    limit_distance = 40     # distance in pixels to know if the robot has reached the end of an segment\n",
    "    \n",
    "\n",
    "    # Ckech if the path planning just came to be done \n",
    "    if path_has_been_done == 1 :\n",
    "        segment_idx = 0\n",
    "        path_has_been_done = 0\n",
    "\n",
    "    # Check if the position has reached the end segment point\n",
    "    if distance(position, path[segment_idx +1]) < limit_distance : \n",
    "        segment_idx += 1\n",
    "        print('let s change segemnt')\n",
    "        if segment_idx == len(path)-1 :\n",
    "            has_finished = 1\n",
    "            print('End')\n",
    "\n",
    "    # Check if the Thymio has reached the end \n",
    "    if has_finished == 1:\n",
    "        motorL = 0\n",
    "        motorR = 0\n",
    "        return motorL, motorR, has_finished, carrot\n",
    "    else : \n",
    "        # step(1)\n",
    "        projection = position + np.array([d_projection*math.cos(teta), d_projection*math.sin(teta)])\n",
    "        # step(2)\n",
    "        A = vector_compute(path[segment_idx], projection)\n",
    "        B = vector_compute(path[segment_idx], path[segment_idx+1])\n",
    "        Bnormal = B / np.linalg.norm(B)\n",
    "        #step(3)\n",
    "        sp = abs(np.dot(A,Bnormal))\n",
    "        maxsp = distance(path[segment_idx],path[segment_idx+1])\n",
    "        if sp >maxsp : \n",
    "            sp = maxsp\n",
    "        #step(4)\n",
    "        carrot = path[segment_idx] + Bnormal * sp\n",
    "        \n",
    "        \n",
    "        motorL, motorR = go_to_carrot(position, carrot, teta, margin)\n",
    "        return motorL, motorR, has_finished, carrot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first parameter to adjust is d_projection, which determines the horizon of the Thymio. If it's too short, the Thymio will react at the last moment during turns. If it's too long, the Thymio will be less responsive.\n",
    "\n",
    "The second parameters to tune are KPteta and KDteta, which are experimentally determined. If KPteta too large, the Thymio follow line behavior may oscillate; if it's too low, the Thymio will turn too slowly. \n",
    "\n",
    "Overall, there has to be harmony between KPteta, d_projection, and the Gain from the local avoidance to have a well-functioning line follower and effective local avoidance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII . Test\n",
    "<a class=\"anchor\" id=\"Test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the folder, you will find a video of the test. Here are some explanations on how to use the robot:\n",
    "\n",
    "As depicted, the setup comprises a white A0 paper on which we arranged Aruco tags to delimit the map. Obstacles are constructed using black paper, and two additional Aruco tags are strategically placed to assist in determining the Thymio's position and the target position.\n",
    "\n",
    "To conduct a test, begin by setting up the arrangement and running the main program. Utilizing the sliders, you can select the threshold to ensure clear images even when there are changes in luminosity. Subsequently, you can observe the chosen path among all the possible paths. Close the window, and the Thymio will start automatically.\n",
    "\n",
    "During the test, you have the following options:\n",
    "\n",
    "- Introduce an obstacle on the map, the robot will navigate to avoid collisions.\n",
    "- To simulate kidnapping the robot, press the forward button. Subsequently, reposition the Thymio and press 'k' on your keyboard to restart the Thymio.\n",
    "- If you obstruct the camera's view, the Thymio will continue its operation seamlessly, thanks to the Kalman filter.\n",
    "\n",
    "Now, you can patiently wait for the Thymio to reach its destination. \n",
    "On the display, you can observe the images captured by the Thymio. The red points and red vector represent the position and orientation calculated using the Kalman filter. The blue point signifies the location of the carrot, while the green points indicate the waypoints along the best path.\n",
    "Upon completion, the Kalman data is computed and stored in a file that provides a comparison between camera data and Kalman data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII . Conclusion\n",
    "<a class=\"anchor\" id=\"Conclusion\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project on enhancing a Thymio robot's navigation with vision and a Kalman filter was not only a technical success but also a profound lesson in teamwork. We successfully integrated diverse code components, learned the intricacies of Git for version control, and honed our collaboration skills. Each team member's contribution was vital, blending unique skills to achieve our common goal. This experience has been invaluable, teaching us the importance of effective communication, patience, and mutual respect in a technical team setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
